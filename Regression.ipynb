{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is Simple Linear Regression?**\n",
        "\n",
        "**Answer:**\n",
        "Simple Linear Regression (SLR) is a supervised learning technique used to model the linear relationship between a **single independent variable (X)** and a **dependent variable (Y)**. The goal is to find the best-fitting straight line (regression line) that minimizes the error between the predicted and actual values of Y. The model is represented by the equation:\n",
        "\n",
        "$$\n",
        "Y = mX + c\n",
        "$$\n",
        "\n",
        "Where `m` is the slope and `c` is the intercept.\n",
        "\n",
        "---\n",
        "\n",
        "**2. What are the key assumptions of Simple Linear Regression?**\n",
        "\n",
        "**Answer:**\n",
        "The effectiveness of SLR relies on several assumptions:\n",
        "\n",
        "1. **Linearity**: The relationship between X and Y is linear.\n",
        "2. **Independence**: The residuals (errors) are independent of each other.\n",
        "3. **Homoscedasticity**: Constant variance of residuals across values of X.\n",
        "4. **Normality**: The residuals are normally distributed.\n",
        "5. **No multicollinearity**: (Not relevant in SLR but important in MLR).\n",
        "\n",
        "Violating these assumptions can lead to biased or inefficient estimates.\n",
        "\n",
        "---\n",
        "\n",
        "**3. What does the coefficient 'm' represent in Y = mX + c?**\n",
        "\n",
        "**Answer:**\n",
        "The coefficient **‘m’** is the **slope** of the regression line. It quantifies the change in the dependent variable Y for a **one-unit increase in the independent variable X**. A positive m indicates a direct relationship, while a negative m indicates an inverse relationship.\n",
        "\n",
        "---\n",
        "\n",
        "**4. What does the intercept 'c' represent in Y = mX + c?**\n",
        "\n",
        "**Answer:**\n",
        "The intercept **‘c’** is the value of Y when X = 0. It represents the point where the regression line crosses the Y-axis. Conceptually, it provides a baseline or reference point for prediction when the input variable has no contribution.\n",
        "\n",
        "---\n",
        "\n",
        "**5. How do we calculate the slope (m) in Simple Linear Regression?**\n",
        "\n",
        "**Answer:**\n",
        "The slope is derived using the **least squares method**, which minimizes the squared differences between actual and predicted Y values. The formula is:\n",
        "\n",
        "$$\n",
        "m = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2}\n",
        "$$\n",
        "\n",
        "This captures the **covariance of X and Y**, normalized by the **variance of X**.\n",
        "\n",
        "---\n",
        "\n",
        "**6. What is the purpose of the least squares method in Simple Linear Regression?**\n",
        "\n",
        "**Answer:**\n",
        "The **least squares method** aims to minimize the **sum of squared residuals** (the vertical distances between observed and predicted Y values). It ensures the regression line has the best fit by reducing the overall prediction error across all data points.\n",
        "\n",
        "---\n",
        "\n",
        "**7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?**\n",
        "\n",
        "**Answer:**\n",
        "**R²**, or the coefficient of determination, measures the **proportion of variance in the dependent variable** that is predictable from the independent variable.\n",
        "\n",
        "* R² = 1 → Perfect fit\n",
        "* R² = 0 → No predictive power\n",
        "  In SLR, a high R² suggests a strong linear relationship, but it does **not imply causation**.\n",
        "\n",
        "\n",
        "**Question 8. What is Multiple Linear Regression?**\n",
        "\n",
        "**Answer:**\n",
        "Multiple Linear Regression (MLR) is an extension of simple linear regression that models the relationship between **one dependent variable (Y)** and **two or more independent variables (X₁, X₂, ..., Xₙ)**.\n",
        "The model is expressed as:\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_n X_n + \\varepsilon\n",
        "$$\n",
        "\n",
        "Each coefficient $\\beta_i$ represents the effect of its corresponding variable $X_i$, assuming all other variables are held constant.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 9. What is the main difference between Simple and Multiple Linear Regression?**\n",
        "\n",
        "**Answer:**\n",
        "The primary difference is the number of independent variables:\n",
        "\n",
        "* **SLR** uses **one independent variable**\n",
        "* **MLR** uses **two or more independent variables**\n",
        "\n",
        "MLR helps in capturing **more complex relationships** and improving model accuracy by considering multiple predictors.\n",
        "\n",
        "---\n",
        "\n",
        "Question 10. What are the key assumptions of Multiple Linear Regression?**\n",
        "\n",
        "**Answer:**\n",
        "MLR relies on the following assumptions:\n",
        "\n",
        "1. **Linearity**: The relationship between independent variables and the dependent variable is linear.\n",
        "2. **Independence of errors**: Residuals are not correlated.\n",
        "3. **Homoscedasticity**: Constant variance of residuals across all levels of predictors.\n",
        "4. **Normality of residuals**: Errors are normally distributed.\n",
        "5. **No multicollinearity**: Independent variables should not be highly correlated with each othe\n",
        "\n",
        "**Question 11. What is heteroscedasticity, and how does it affect MLR results?**\n",
        "\n",
        "**Answer:**\n",
        "**Heteroscedasticity** occurs when the variance of residuals is **not constant** across all levels of the independent variables.\n",
        "This violates the homoscedasticity assumption and leads to:\n",
        "\n",
        "* Biased standard errors\n",
        "* Invalid hypothesis tests (t-tests, F-tests)\n",
        "* Less reliable confidence intervals\n",
        "\n",
        "It is often detected via **residual plots**.\n",
        "\n",
        "**Question 12. How can you improve a Multiple Linear Regression model with high multicollinearity?**\n",
        "\n",
        "**Answer:**\n",
        "To handle multicollinearity (when independent variables are highly correlated), consider:\n",
        "\n",
        "* **Removing or combining correlated predictors**\n",
        "* **Using PCA (Principal Component Analysis)**\n",
        "* **Applying regularization techniques** like Ridge or Lasso regression\n",
        "* **Variance Inflation Factor (VIF)** analysis to detect problematic variables\n",
        "\n",
        "**Question 13. What are some common techniques for transforming categorical variables in regression?**\n",
        "\n",
        "**Answer:**\n",
        "To include categorical variables in a regression model:\n",
        "\n",
        "* **One-Hot Encoding**: Converts categories into binary columns (e.g., 'Red', 'Blue' → \\[1, 0], \\[0, 1])\n",
        "* **Label Encoding**: Assigns numeric labels (use cautiously as it implies order)\n",
        "* **Ordinal Encoding**: For ordered categories (e.g., Low < Medium < High)\n",
        "\n",
        "These techniques help in incorporating non-numeric features into numerical models.\n",
        "\n",
        "\n",
        "**Question 14. What is the role of interaction terms in MLR?**\n",
        "\n",
        "**Answer:**\n",
        "**Interaction terms** capture the **combined effect** of two or more variables on the dependent variable.\n",
        "For instance, the effect of **X₁ on Y** might change depending on **X₂**.\n",
        "Interaction term:\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1 \\cdot X_2)\n",
        "$$\n",
        "\n",
        "Useful when variables interact in a **non-additive** way.\n",
        "\n",
        "\n",
        "**Question 15. How can the interpretation of intercept differ between SLR and MLR?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* In **SLR**, the intercept is the expected value of Y when **X = 0**.\n",
        "* In **MLR**, the intercept represents the expected value of Y when **all independent variables = 0**.\n",
        "  This may not always be interpretable or realistic depending on the variables involved.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 16. What is the significance of the slope in regression analysis?**\n",
        "\n",
        "**Answer:**\n",
        "Each slope (coefficient) in a regression model represents the **average change in the dependent variable Y** for a **one-unit change in the independent variable**, assuming all other variables are constant.\n",
        "It quantifies the **direction and strength** of influence.\n",
        "\n",
        "\n",
        "Question 17. How does the intercept in a regression model provide context for the relationship between variables?**\n",
        "\n",
        "**Answer:**\n",
        "The intercept offers a **baseline value** of the target variable when all predictors are zero. While sometimes it lacks real-world meaning, it is essential for positioning the regression plane or line in multi-dimensional spce\n",
        "\n",
        "**18. What are the limitations of using R² as a sole measure of model performance?**\n",
        "\n",
        "**Answer:**\n",
        "R² only indicates the **proportion of variance explained**, but:\n",
        "\n",
        "* It increases even if we add irrelevant variables.\n",
        "* It doesn't measure **predictive accuracy**.\n",
        "* It doesn't penalize model complexity.\n",
        "\n",
        "Hence, **Adjusted R²**, **RMSE**, and **cross-validation** scores are preferred for holistic evaluation.\n",
        "\n",
        "**19. How would you interpret a large standard error for a regression coefficient?**\n",
        "\n",
        "**Answer:**\n",
        "A **large standard error** indicates high variability in the estimate of the coefficient, meaning the model is **less confident** in that coefficient's value. This may suggest the variable is not significantly contributing to the model.\n",
        "\n",
        "**20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?**\n",
        "\n",
        "**Answer:**\n",
        "Plot residuals vs. predicted values:\n",
        "\n",
        "* If the spread of residuals increases or decreases with predictions, heteroscedasticity is present (e.g., fan or cone shapes).\n",
        "* Important to address it as it leads to **inefficient and biased** coefficient estimates and **invalid hypothesis testing**.\n",
        "\n",
        "\n",
        "**21. What does it mean if a MLR model has a high R² but low adjusted R²?**\n",
        "\n",
        "**Answer:**\n",
        "This means the model has many variables that **do not improve prediction**.\n",
        "\n",
        "* **R²** increases with every variable added.\n",
        "* **Adjusted R²** penalizes unnecessary variables.\n",
        "  If adjusted R² is low, the model may be **overfitted or poorly specified**.\n",
        "\n",
        "**22. Why is it important to scale variables in Multiple Linear Regression?**\n",
        "\n",
        "**Answer:**\n",
        "Scaling (standardization or normalization) ensures:\n",
        "\n",
        "* Variables are on the **same scale**, avoiding dominance by variables with large ranges.\n",
        "* Improves **model stability** and **interpretation of coefficients**, especially in regularized regression (e.g., Ridge, Lasso).\n",
        "\n",
        "**23. What is Polynomial Regression?**\n",
        "\n",
        "**Answer:**\n",
        "Polynomial Regression is a form of linear regression where the **relationship between independent and dependent variables is modeled as an nth-degree polynomial**.\n",
        "Instead of fitting a straight line (as in linear regression), it fits a **curve** to capture nonlinear patterns.\n",
        "Equation:\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\cdots + \\beta_nX^n + \\varepsilon\n",
        "$$\n",
        "\n",
        "\n",
        "**24. How does Polynomial Regression differ from Linear Regression?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* **Linear Regression** fits a straight line (1st-degree polynomial).\n",
        "* **Polynomial Regression** fits a **curved line** by adding powers of X (e.g., X², X³).\n",
        "  It can model more complex, nonlinear relationships.\n",
        "\n",
        "\n",
        "**25. When is Polynomial Regression used?**\n",
        "\n",
        "**Answer:**\n",
        "Polynomial Regression is used when:\n",
        "\n",
        "* The data shows **nonlinear patterns** that a straight line cannot capture.\n",
        "* The residuals from a linear regression indicate **non-random patterns**.\n",
        "  Common in modeling growth curves, market trends, or physical phenomena.\n",
        "\n",
        "**26. What is the general equation for Polynomial Regression?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\cdots + \\beta_nX^n + \\varepsilon\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\beta_i$ are the coefficients\n",
        "* $X^n$ are the polynomial terms\n",
        "* $\\varepsilon$ is the error term\n",
        "\n",
        "\n",
        "**27. Can Polynomial Regression be applied to multiple variables?**\n",
        "\n",
        "**Answer:**\n",
        "Yes. Polynomial regression can be extended to **multiple variables**, resulting in **interaction and power terms** for each variable.\n",
        "For example, with X and Z:\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3X^2 + \\beta_4XZ + \\beta_5Z^2\n",
        "$$\n",
        "\n",
        "This is also known as **multivariate polynomial regression**.\n",
        "\n",
        "\n",
        "**28. What are the limitations of Polynomial Regression?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* **Overfitting**: High-degree polynomials can model noise instead of the trend.\n",
        "* **Interpretability**: As the degree increases, the model becomes complex and hard to explain.\n",
        "* **Instability**: Slight changes in data can cause large swings in the curve (especially at boundaries — known as Runge’s phenomenon).\n",
        "\n",
        "\n",
        "**29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* **Cross-validation (k-fold)**: Helps detect overfitting.\n",
        "* **Adjusted R²**: Penalizes unnecessary complexity.\n",
        "* **AIC/BIC (Akaike/Bayesian Information Criteria)**: Lower values indicate a better trade-off between fit and complexity.\n",
        "* **Residual plots**: Check for randomness in residuals.\n",
        "\n",
        "**30. Why is visualization important in Polynomial Regression?**\n",
        "\n",
        "**Answer:**\n",
        "Visualization helps to:\n",
        "\n",
        "* **Understand** the shape and flexibility of the fitted curve.\n",
        "* Detect **underfitting (too simple)** or **overfitting (too complex)** visually.\n",
        "* Communicate results intuitively to stakeholders.\n",
        "  Plots of data + fitted curve + residuals give crucial insights into model behavior.\n",
        "\n",
        "\n",
        "**31 How is Polynomial Regression implemented in Python **.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_TGnK9EjaDff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "X = [[1], [2], [3], [4]]\n",
        "y = [3, 6, 11, 18]\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_poly)\n"
      ],
      "metadata": {
        "id": "0rVzX-cZaC7m"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lSJ1g9qaBP4"
      },
      "outputs": [],
      "source": []
    }
  ]
}